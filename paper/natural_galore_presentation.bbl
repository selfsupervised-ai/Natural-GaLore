\begin{thebibliography}{19}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amari(1998)]{amariNaturalGradientWorks1998}
Shun-ichi Amari.
\newblock Natural gradient works efficiently in learning.
\newblock \emph{Neural Computation}, 10\penalty0 (2):\penalty0 251--276, 1998.
\newblock URL \url{https://ieeexplore.ieee.org/document/6790500}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brownLanguageModelsAre2020}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  1877--1901, 2020.
\newblock URL \url{https://arxiv.org/abs/2005.14165}.

\bibitem[Chen et~al.(2016)Chen, Xu, Zhang, and Guestrin]{chenTrainingDeepNets2016}
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
\newblock Training deep nets with sublinear memory cost.
\newblock In \emph{Proceedings of the 20th International Conference on Machine Learning (ICML)}, 2016.
\newblock URL \url{https://arxiv.org/abs/1604.06174}.

\bibitem[Erdogan et~al.(2024)Erdogan, Lee, Jha, Kim, Tabrizi, Moon, Hooper, Anumanchipalli, Keutzer, and Gholami]{erdogan2024tinyagent}
Lutfi~Eren Erdogan, Nicholas Lee, Siddharth Jha, Sehoon Kim, Ryan Tabrizi, Suhong Moon, Coleman Hooper, Gopala Anumanchipalli, Kurt Keutzer, and Amir Gholami.
\newblock {TinyAgent}: Function calling at the edge.
\newblock \emph{arXiv preprint arXiv:2409.00608}, 2024.
\newblock URL \url{https://arxiv.org/abs/2409.00608}.

\bibitem[He et~al.(2021)He, Gao, and Chen]{he2021debertav3}
Pengcheng He, Jianfeng Gao, and Weizhu Chen.
\newblock {DeBERTaV3}: Improving {DeBERTa} using {ELECTRA}-style pre-training with gradient-disentangled embedding sharing.
\newblock \emph{arXiv preprint arXiv:2111.09543}, 2021.
\newblock URL \url{https://arxiv.org/abs/2111.09543}.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, and Chen]{huLoRALowRankAdaptation2021}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen.
\newblock {LoRA}: Low-rank adaptation of large language models.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://arxiv.org/abs/2106.09685}.

\bibitem[Huang et~al.(2019)Huang, Cheng, Bapna, Firat, Chen, Chen, Hu, Shen, Krikun, Wu, et~al.]{huangGPipeEfficientTraining2019}
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Menglong Chen, Denny Chen, Zhifeng Hu, Yuxin Shen, Maxim Krikun, Yonghui Wu, et~al.
\newblock {GPipe}: Efficient training of giant neural networks using pipeline parallelism.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~32, pp.\  103--112, 2019.
\newblock URL \url{https://arxiv.org/abs/1811.06965}.

\bibitem[Kim et~al.(2023)Kim, Moon, Tabrizi, Lee, Mahoney, Keutzer, and Gholami]{kim2023llmcompiler}
Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas Lee, Michael~W Mahoney, Kurt Keutzer, and Amir Gholami.
\newblock An {LLM} compiler for parallel function calling.
\newblock \emph{arXiv preprint arXiv:2312.04511}, 2023.
\newblock URL \url{https://arxiv.org/abs/2312.04511}.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingmaAdamMethodStochastic2014}
Diederik~P Kingma and Jimmy Ba.
\newblock {Adam}: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.
\newblock URL \url{https://arxiv.org/abs/1412.6980}.

\bibitem[Lialin \& Schatz(2023)Lialin and Schatz]{lialinReLoRAHighRankTraining2023}
Vladimir Lialin and Arthur Schatz.
\newblock {ReLoRA}: Low-rank fine-tuning reloaded.
\newblock \emph{arXiv preprint arXiv:2307.09769}, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.09769}.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.
\newblock URL \url{https://arxiv.org/abs/1711.05101}.

\bibitem[Martens(2020)]{martens2020new}
James Martens.
\newblock New insights and perspectives on the natural gradient method.
\newblock \emph{Journal of Machine Learning Research}, 21:\penalty0 1--76, 2020.
\newblock URL \url{https://www.jmlr.org/papers/volume21/17-678/17-678.pdf}.

\bibitem[Opsahl-Ong et~al.(2024)Opsahl-Ong, Ryan, Purtell, Broman, Potts, Zaharia, and Khattab]{opsahlong2024optimizinginstructionsdemonstrationsmultistage}
Krista Opsahl-Ong, Michael~J Ryan, Josh Purtell, David Broman, Christopher Potts, Matei Zaharia, and Omar Khattab.
\newblock Optimizing instructions and demonstrations for multi-stage language model programs, 2024.
\newblock URL \url{https://arxiv.org/abs/2406.11695}.

\bibitem[Rajbhandari et~al.(2020)Rajbhandari, Rasley, Ruwase, and He]{rajbhandariZeROMemoryOptimizations2020}
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.
\newblock {ZeRO}: Memory optimizations toward training trillion parameter models.
\newblock In \emph{Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis}, pp.\  1--16, 2020.
\newblock URL \url{https://arxiv.org/abs/1910.02054}.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro]{shoeybiMegatronLMTuningScaling2019}
Mohammad Shoeybi, Mostofa Patwary, Rohan Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.
\newblock {Megatron-LM}: Training multi-billion parameter language models using model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.
\newblock URL \url{https://arxiv.org/abs/1909.08053}.

\bibitem[Xia et~al.(2024)Xia, Peng, Chen, Li, He, Yang, and Ma]{xiaChainLoRAEfficient2024}
Tianxiang Xia, Hao Peng, Zheyu Chen, Lemao Li, Zhiyuan He, Zhen Yang, and Wei-Ying Ma.
\newblock Chain-of-thought lora: Efficient adaptation of large language models.
\newblock In \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2024.
\newblock To appear.

\bibitem[Yang et~al.(2024)Yang, Wang, Lu, Liu, Le, Zhou, and Chen]{yang2024largelanguagemodelsoptimizers}
Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc~V. Le, Denny Zhou, and Xinyun Chen.
\newblock Large language models as optimizers, 2024.
\newblock URL \url{https://arxiv.org/abs/2309.03409}.

\bibitem[Zhao et~al.(2024)Zhao, Zhang, Chen, Wang, Anandkumar, and Tian]{zhao2024galore}
Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian.
\newblock {GaLore}: Memory-efficient {LLM} training by gradient low-rank projection.
\newblock \emph{arXiv preprint arXiv:2403.03507}, 2024.
\newblock URL \url{https://arxiv.org/abs/2403.03507}.

\bibitem[Zhao et~al.(2020)Zhao, Sun, Wang, Zhou, Guo, and Smola]{zhaoExtendingTorchElasticStateful2020}
Tianshi Zhao, Zhen Sun, Xiaodong Wang, Fei Zhou, Yang Guo, and Alexander~J Smola.
\newblock Extending torchelastic for stateful training jobs.
\newblock \emph{arXiv preprint arXiv:2006.06873}, 2020.
\newblock URL \url{https://arxiv.org/abs/2006.06873}.

\end{thebibliography}
