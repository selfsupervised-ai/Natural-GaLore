\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{raffelExploringLimitsTransfer2020,touvronLlamaOpenFoundation2023,chowdheryPaLMScalingLanguage2022}
\citation{zhao2024galore}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{huangGPipeEfficientTraining2019}
\citation{shoeybiMegatronLMTuningScaling2019}
\citation{chenTrainingDeepNets2016}
\citation{rajbhandariZeROMemoryOptimizations2020}
\citation{zhaoExtendingTorchElasticStateful2020}
\citation{touvronLlamaOpenFoundation2023}
\citation{brownLanguageModelsAre2020}
\citation{jiangMistralEfficientComposable2023}
\citation{raeScalingLanguageModels2021}
\citation{dingDeltaTuningComprehensive2022}
\citation{huLoRALowRankAdaptation2021}
\citation{renduchintalaTiedLoraEnhacingParameter2023,shengSLoRAServingThousands2023,zhangLORAFAMEMORYEFFICIENTLOWRANK,xiaChainLoRAEfficient2024}
\citation{wangMultiLoRADemocratizingLoRA2023}
\citation{dettmersQLoRAEfficientFinetuning2023}
\citation{lialinReLoRAHighRankTraining2023}
\@writefile{toc}{\contentsline {paragraph}{Parallel and Distributed Training Techniques}{2}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Parameter-Efficient Fine-Tuning}{2}{section*.2}\protected@file@percent }
\citation{xiaChainLoRAEfficient2024}
\citation{lialinReLoRAHighRankTraining2023}
\citation{zhao2024galore}
\citation{zhaoZerOInitializationInitializing2022,cossonLowRankGradientDescent2023,yang2023spectral}
\citation{wangATOMOCommunicationefficientLearning,vogelsPowerGossipPracticalLowRank2020}
\citation{gooneratneLowrankGradientApproximation2020,huangLowRankGradientDescent2023}
\citation{zhao2024galore}
\citation{martens2015optimizing}
\@writefile{toc}{\contentsline {paragraph}{Gradient Low-Rank Projection (GaLore)}{3}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Our Approach}{3}{section*.4}\protected@file@percent }
\citation{lin2022randomized}
\@writefile{toc}{\contentsline {section}{\numberline {2}Accelerating GaLore with Natural Gradients}{4}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Next Token Prediction}{4}{subsection.2.1}\protected@file@percent }
\newlabel{eq:cross_entropy_loss}{{4}{4}{Next Token Prediction}{equation.2.4}{}}
\newlabel{eq:cross_entropy_loss@cref}{{[equation][4][]4}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Low-Rank Gradient Descent}{4}{subsection.2.2}\protected@file@percent }
\newlabel{eq:taylor_series_expansion}{{5}{4}{Low-Rank Gradient Descent}{equation.2.5}{}}
\newlabel{eq:taylor_series_expansion@cref}{{[equation][5][]5}{[1][4][]4}}
\citation{martensNewPerspectiveNatural2014}
\citation{martens2020new}
\citation{kingmaAdamMethodStochastic2014}
\citation{loshchilov2017decoupled}
\citation{loshchilov2017decoupled}
\citation{martens2020new}
\citation{martens2020new}
\citation{amariNaturalGradientWorks1998}
\newlabel{eq:optimal_direction}{{8}{5}{Low-Rank Gradient Descent}{equation.2.8}{}}
\newlabel{eq:optimal_direction@cref}{{[equation][8][]8}{[1][5][]5}}
\newlabel{eq:gradient_descent_update}{{9}{5}{Low-Rank Gradient Descent}{equation.2.9}{}}
\newlabel{eq:gradient_descent_update@cref}{{[equation][9][]9}{[1][5][]5}}
\newlabel{eq:adam_update}{{12}{5}{Low-Rank Gradient Descent}{equation.2.12}{}}
\newlabel{eq:adam_update@cref}{{[equation][12][]12}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Natural GaLore\xspace  and Fisher Efficiency}{5}{subsection.2.3}\protected@file@percent }
\citation{martens2020new}
\citation{zhao2024galore}
\newlabel{eq:variance_reduction}{{13}{6}{\lowrank and Fisher Efficiency}{equation.2.13}{}}
\newlabel{eq:variance_reduction@cref}{{[equation][13][]13}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Natural Gradient Transform}{6}{subsection.2.4}\protected@file@percent }
\citation{raffelExploringLimitsTransfer2020}
\citation{lialinReLoRAHighRankTraining2023}
\citation{shazeerGLUVariantsImprove2020,touvronLlamaOpenFoundation2023}
\citation{zhao2024galore}
\citation{warstadt2019neural}
\citation{dolan2005automatically}
\citation{cer2017semeval}
\citation{dagan2005pascal}
\citation{williams2018broad}
\citation{rajpurkar2016squad}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments}{7}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Pre-training on the C4 Dataset}{7}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  {Training and Validation log Perplexity for Llama 1.1B }}}{7}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:train_loss}{{1}{7}{\small {Training and Validation log Perplexity for Llama 1.1B }}{figure.caption.5}{}}
\newlabel{fig:train_loss@cref}{{[figure][1][]1}{[1][7][]7}}
\citation{huLoRALowRankAdaptation2021}
\citation{erdogan2024tinyagent}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  {Comparison of \textit  {Natural GaLore\xspace  } with other low-rank algorithms on pre-training various sizes of LLaMA models on the C4 dataset. Validation log perplexity is reported (averaged over 5 runs), along with a memory estimate (in gigabytes) of the total parameters and optimizer states based on BF16 format.}}}{8}{table.1}\protected@file@percent }
\newlabel{tab:lora_compare_llama}{{1}{8}{\small {Comparison of \textit {\lowrank } with other low-rank algorithms on pre-training various sizes of LLaMA models on the C4 dataset. Validation log perplexity is reported (averaged over 5 runs), along with a memory estimate (in gigabytes) of the total parameters and optimizer states based on BF16 format.}}{table.1}{}}
\newlabel{tab:lora_compare_llama@cref}{{[table][1][]1}{[1][7][]8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Fine-Tuning RoBERTa-Base on the GLUE Benchmark}{8}{subsection.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  {Evaluating \textit  {Natural GaLore\xspace  } for memory-efficient fine-tuning on the GLUE benchmark using pre-trained RoBERTa-Base. We report the average score of all tasks. Memory consumption is reported in millions of parameters (M).}}}{8}{table.2}\protected@file@percent }
\newlabel{tab:fine_tuning}{{2}{8}{\small {Evaluating \textit {\lowrank } for memory-efficient fine-tuning on the GLUE benchmark using pre-trained RoBERTa-Base. We report the average score of all tasks. Memory consumption is reported in millions of parameters (M).}}{table.2}{}}
\newlabel{tab:fine_tuning@cref}{{[table][2][]2}{[1][8][]8}}
\citation{kim2023llmcompiler}
\citation{erdogan2024tinyagent}
\citation{erdogan2024tinyagent}
\citation{he2021debertav3}
\citation{erdogan2024tinyagent}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Fine-Tuning TinyLlama 1.1B for Function Calling in Advanced Agentic Systems}{9}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{TinyAgent Dataset}{9}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fine-Tuning Procedure}{9}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Results and Discussion}{9}{table.3}\protected@file@percent }
\bibdata{iclr_template/iclr2025_conference}
\bibcite{amariNaturalGradientWorks1998}{{1}{1998}{{Amari}}{{}}}
\bibcite{brownLanguageModelsAre2020}{{2}{2020}{{Brown et~al.}}{{Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.}}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  Latency, size, and success rate of TinyAgent models before and after quantization. Latency is the end-to-end latency of the function calling planner, including the prompt processing time and generation.}}{10}{table.3}\protected@file@percent }
\newlabel{table:t2}{{3}{10}{Latency, size, and success rate of TinyAgent models before and after quantization. Latency is the end-to-end latency of the function calling planner, including the prompt processing time and generation}{table.3}{}}
\newlabel{table:t2@cref}{{[table][3][]3}{[1][9][]10}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{10}{section.4}\protected@file@percent }
\bibcite{cer2017semeval}{{3}{2017}{{Cer et~al.}}{{Cer, Diab, Agirre, Lopez-Gazpio, and Specia}}}
\bibcite{chenTrainingDeepNets2016}{{4}{2016}{{Chen et~al.}}{{Chen, Xu, Zhang, and Guestrin}}}
\bibcite{chowdheryPaLMScalingLanguage2022}{{5}{2022}{{Chowdhery et~al.}}{{Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, et~al.}}}
\bibcite{cossonLowRankGradientDescent2023}{{6}{2023}{{Cosson et~al.}}{{Cosson, Lecouat, Varre, d'Ascoli, and Biroli}}}
\bibcite{dagan2005pascal}{{7}{2006}{{Dagan et~al.}}{{Dagan, Glickman, and Magnini}}}
\bibcite{dettmersQLoRAEfficientFinetuning2023}{{8}{2023}{{Dettmers et~al.}}{{Dettmers, Pagnoni, Holtzman, and Zettlemoyer}}}
\bibcite{dingDeltaTuningComprehensive2022}{{9}{2022}{{Ding et~al.}}{{Ding, Zheng, Wang, Chen, Liu, Zheng, Qiu, Shen, Ding, and Tang}}}
\bibcite{dolan2005automatically}{{10}{2005}{{Dolan \& Brockett}}{{Dolan and Brockett}}}
\bibcite{erdogan2024tinyagent}{{11}{2024}{{Erdogan et~al.}}{{Erdogan, Lee, Jha, Kim, Tabrizi, Moon, Hooper, Anumanchipalli, Keutzer, and Gholami}}}
\bibcite{gooneratneLowrankGradientApproximation2020}{{12}{2020}{{Gooneratne et~al.}}{{Gooneratne, Wang, Guo, Kanuparthi, Rajan, and Jayasumana}}}
\bibcite{he2021debertav3}{{13}{2021}{{He et~al.}}{{He, Gao, and Chen}}}
\bibcite{huLoRALowRankAdaptation2021}{{14}{2022}{{Hu et~al.}}{{Hu, Shen, Wallis, Allen-Zhu, Li, Wang, and Chen}}}
\bibcite{huangGPipeEfficientTraining2019}{{15}{2019}{{Huang et~al.}}{{Huang, Cheng, Bapna, Firat, Chen, Chen, Hu, Shen, Krikun, Wu, et~al.}}}
\bibcite{jiangMistralEfficientComposable2023}{{16}{2023}{{Jiang et~al.}}{{Jiang, Li, Gan, Liu, Chen, Zhu, Li, Wang, Wang, and Liu}}}
\bibcite{kim2023llmcompiler}{{17}{2023}{{Kim et~al.}}{{Kim, Moon, Tabrizi, Lee, Mahoney, Keutzer, and Gholami}}}
\bibcite{kingmaAdamMethodStochastic2014}{{18}{2014}{{Kingma \& Ba}}{{Kingma and Ba}}}
\bibcite{lialinReLoRAHighRankTraining2023}{{19}{2023}{{Lialin \& Schatz}}{{Lialin and Schatz}}}
\bibcite{lin2022randomized}{{20}{2022}{{Lin et~al.}}{{Lin, Zhu, and Mao}}}
\bibcite{loshchilov2017decoupled}{{21}{2017}{{Loshchilov \& Hutter}}{{Loshchilov and Hutter}}}
\bibcite{martensNewPerspectiveNatural2014}{{22}{2014}{{Martens}}{{}}}
\bibcite{martens2020new}{{23}{2020}{{Martens}}{{}}}
\bibcite{martens2015optimizing}{{24}{2015}{{Martens \& Grosse}}{{Martens and Grosse}}}
\bibcite{raeScalingLanguageModels2021}{{25}{2021}{{Rae et~al.}}{{Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young, et~al.}}}
\bibcite{raffelExploringLimitsTransfer2020}{{26}{2020}{{Raffel et~al.}}{{Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu}}}
\bibcite{rajbhandariZeROMemoryOptimizations2020}{{27}{2020}{{Rajbhandari et~al.}}{{Rajbhandari, Rasley, Ruwase, and He}}}
\bibcite{rajpurkar2016squad}{{28}{2016}{{Rajpurkar et~al.}}{{Rajpurkar, Zhang, Lopyrev, and Liang}}}
\bibcite{renduchintalaTiedLoraEnhacingParameter2023}{{29}{2023}{{Renduchintala et~al.}}{{Renduchintala, Rodriguez, and Creutz}}}
\bibcite{shazeerGLUVariantsImprove2020}{{30}{2020}{{Shazeer}}{{}}}
\bibcite{shengSLoRAServingThousands2023}{{31}{2023}{{Sheng et~al.}}{{Sheng, Han, Zhu, Yang, Sun, and Zhou}}}
\bibcite{shoeybiMegatronLMTuningScaling2019}{{32}{2019}{{Shoeybi et~al.}}{{Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro}}}
\bibcite{touvronLlamaOpenFoundation2023}{{33}{2023}{{Touvron et~al.}}{{Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.}}}
\bibcite{vogelsPowerGossipPracticalLowRank2020}{{34}{2020}{{Vogels et~al.}}{{Vogels, Jaggi, and Patrini}}}
\bibcite{wangATOMOCommunicationefficientLearning}{{35}{2018}{{Wang et~al.}}{{Wang, Joshi, Ghosh, and Poor}}}
\bibcite{wangMultiLoRADemocratizingLoRA2023}{{36}{2023}{{Wang et~al.}}{{Wang, Bai, and Ananiadou}}}
\bibcite{warstadt2019neural}{{37}{2019}{{Warstadt et~al.}}{{Warstadt, Singh, and Bowman}}}
\bibcite{williams2018broad}{{38}{2018}{{Williams et~al.}}{{Williams, Nangia, and Bowman}}}
\bibcite{xiaChainLoRAEfficient2024}{{39}{2024}{{Xia et~al.}}{{Xia, Peng, Chen, Li, He, Yang, and Ma}}}
\bibcite{yang2023spectral}{{40}{2023}{{Yang et~al.}}{{Yang, Hu, Xia, Socher, and Li}}}
\bibcite{zhangLORAFAMEMORYEFFICIENTLOWRANK}{{41}{2023}{{Zhang et~al.}}{{}}}
\bibcite{zhao2024galore}{{42}{2024{a}}{{Zhao et~al.}}{{Zhao, Zhang, Chen, Wang, Anandkumar, and Tian}}}
\bibcite{huangLowRankGradientDescent2023}{{43}{2024{b}}{{Zhao et~al.}}{{Zhao, Zhang, et~al.}}}
\bibcite{zhaoZerOInitializationInitializing2022}{{44}{2022}{{Zhao et~al.}}{{Zhao, Li, and Ma}}}
\bibcite{zhaoExtendingTorchElasticStateful2020}{{45}{2020}{{Zhao et~al.}}{{Zhao, Sun, Wang, Zhou, Guo, and Smola}}}
\bibstyle{iclr_template/iclr2025_conference}
\gdef \@abspage@last{13}
