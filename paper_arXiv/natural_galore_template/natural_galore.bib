@article{amariNaturalGradientWorks1998,
 author = {Amari, Shun-ichi},
 journal = {Neural Computation},
 title = {Natural Gradient Works Efficiently in Learning},
 year = {1998}
}

@article{anilMemoryEfficientAdaptive2019,
 author = {Anil, Rohan and Gupta, Vineet and Passos, Alexandre and Shazeer, Noam},
 journal = {arXiv preprint arXiv:1901.11150},
 title = {Memory-Efficient Adaptive Optimization},
 year = {2019}
}

@inproceedings{brownLanguageModelsAre2020,
 author = {Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Language Models are Few-Shot Learners},
 year = {2020}
}

@inproceedings{cer2017semeval,
 author = {Cer, Daniel and Diab, Mona and Agirre, Eneko and Lopez-Gazpio, I{\~n}igo and Specia, Lucia},
 booktitle = {Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)},
 title = {SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation},
 year = {2017}
}

@inproceedings{chaudhryContinualLearningLowrank2020,
 author = {Chaudhry, Arslan and Dokania, Puneet K and Torr, Philip HS},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Continual Learning with Tiny Memories in Low-Rank Orthogonal Subspaces},
 year = {2020}
}

@inproceedings{chenFastLowrankEstimation2015,
 author = {Chen, Yu and Wainwright, Martin J},
 booktitle = {International Conference on Machine Learning},
 title = {Fast Low-Rank Matrix Estimation without the Condition Number},
 year = {2017}
}

@article{chenNonConvexProjectedGradient2019,
 author = {Chen, Yuxin and Wainwright, Martin J},
 journal = {IEEE Transactions on Information Theory},
 title = {Non-Convex Projected Gradient Descent for General Low-Rank Matrix Recovery},
 year = {2019}
}

@inproceedings{chenTrainingDeepNets2016,
 author = {Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
 booktitle = {Proceedings of the 20th International Conference on Machine Learning (ICML)},
 title = {Training Deep Nets with Sublinear Memory Cost},
 year = {2016}
}

@article{chowdheryPaLMScalingLanguage2022,
 author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung~Won and Sutton, Charles and Gehrmann, Sebastian and others},
 journal = {arXiv preprint arXiv:2204.02311},
 title = {{PaLM}: Scaling Language Modeling with Pathways},
 year = {2022}
}

@article{cossonLowRankGradientDescent2023,
 author = {Cosson, Victor and Lecouat, Baptiste and Varre, Arthur and d'Ascoli, St{\'e}phane and Biroli, Giulio},
 journal = {arXiv preprint arXiv:2301.12995},
 title = {Low-Rank Gradient Descent Converges and Generalizes},
 year = {2023}
}

@inproceedings{dagan2005pascal,
 author = {Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
 booktitle = {Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Textual Entailment},
 publisher = {Springer},
 title = {The {PASCAL} Recognising Textual Entailment Challenge},
 year = {2006}
}

@inproceedings{dettmers8bitOptimizersBlockwise2021,
 author = {Dettmers, Tim and Lewis, Mike and Shleifer, Sam and Zettlemoyer, Luke},
 booktitle = {International Conference on Learning Representations},
 title = {8-bit Optimizers via Block-wise Quantization},
 year = {2022}
}

@article{dettmersQLoRAEfficientFinetuning2023,
 author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
 journal = {arXiv preprint arXiv:2305.14314},
 title = {{QLoRA}: Efficient Finetuning of Quantized {LLMs}},
 year = {2023}
}

@inproceedings{dingDeltaTuningComprehensive2022,
 author = {Ding, Ning and Zheng, Xiang and Wang, Yujia and Chen, Yifei and Liu, Yichi and Zheng, Haitao and Qiu, Xipeng and Shen, Yujun and Ding, Bolin and Tang, Jie},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models},
 year = {2022}
}

@inproceedings{dolan2005automatically,
 author = {Dolan, William B and Brockett, Chris},
 booktitle = {Proceedings of the Third International Workshop on Paraphrasing (IWP2005)},
 title = {Automatically Constructing a Corpus of Sentential Paraphrases},
 year = {2005}
}

@article{dosovitskiy2021an,
 author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
 journal = {arXiv preprint arXiv:2010.11929},
 title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
 year = {2020}
}

@article{erdogan2024tinyagent,
 author = {Erdogan, Lutfi Eren and Lee, Nicholas and Jha, Siddharth and Kim, Sehoon and Tabrizi, Ryan and Moon, Suhong and Hooper, Coleman and Anumanchipalli, Gopala and Keutzer, Kurt and Gholami, Amir},
 journal = {arXiv preprint arXiv:2409.00608},
 title = {{TinyAgent}: Function Calling at the Edge},
 year = {2024}
}

@article{gooneratneLowrankGradientApproximation2020,
 author = {Gooneratne, Shamal and Wang, Meng and Guo, Zhili and Kanuparthi, Vamsi Krishna and Rajan, Dinesh and Jayasumana, Anura P},
 journal = {arXiv preprint arXiv:2011.01679},
 title = {Low-Rank Gradient Approximation for Multi-Task Learning},
 year = {2020}
}

@inproceedings{gur-ariGradientDescentHappens2018,
 author = {Gur-Ari, Guy and Roberts, Daniel A and Dyer, Ethan},
 booktitle = {International Conference on Machine Learning},
 title = {Gradient Descent Happens in a Tiny Subspace},
 year = {2018}
}

@article{haoFloraLowRankAdapters2024,
 author = {Hao, Yuning and Gu, Shixiang and Liang, Chen},
 journal = {arXiv preprint arXiv:2306.17878},
 title = {{FLORA}: Fine-grained Low-Rank Adaptation},
 year = {2023}
}

@article{he2021debertav3,
 author = {He, Pengcheng and Gao, Jianfeng and Chen, Weizhu},
 journal = {arXiv preprint arXiv:2111.09543},
 title = {{DeBERTaV3}: Improving {DeBERTa} using {ELECTRA}-Style Pre-Training with Gradient-Disentangled Embedding Sharing},
 year = {2021}
}

@inproceedings{ho2020denoising,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Denoising Diffusion Probabilistic Models},
 year = {2020}
}

@inproceedings{huangGPipeEfficientTraining2019,
 author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Menglong and Chen, Denny and Hu, Zhifeng and Shen, Yuxin and Krikun, Maxim and Wu, Yonghui and others},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {{GPipe}: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
 year = {2019}
}

@article{huangLowRankGradientDescent2023,
 author = {Zhao, Jiawei and Zhang, Zhenyu and others},
 journal = {International Conference on Machine Learning},
 title = {GaLore: Low-Rank Gradient Descent: Fast Convergence and Low Memory Cost},
 year = {2024}
}

@inproceedings{huLoRALowRankAdaptation2021,
 author = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Chen, Weizhu},
 booktitle = {International Conference on Learning Representations},
 title = {{LoRA}: Low-Rank Adaptation of Large Language Models},
 year = {2022}
}

@article{jiangMistralEfficientComposable2023,
 author = {Jiang, Ye and Li, Pengcheng and Gan, Zhe and Liu, Jianfeng and Chen, Dongdong and Zhu, Xiaodong and Li, Zhangyang and Wang, Lijuan and Wang, Jianfeng and Liu, Zicheng},
 journal = {arXiv preprint arXiv:2305.15334},
 title = {{Mistral}: Efficient Composable Inference for Large Language Models},
 year = {2023}
}

@article{kamalakaraExploringLowRank2022,
 author = {Kamalakara, Himanshu and Kudugunta, Sneha and Sahu, Rohit Prakash and He, He},
 journal = {arXiv preprint arXiv:2203.07261},
 title = {Exploring Low-Rank Training of Deep Neural Networks},
 year = {2022}
}

@article{kim2023llmcompiler,
 author = {Kim, Sehoon and Moon, Suhong and Tabrizi, Ryan and Lee, Nicholas and Mahoney, Michael W and Keutzer, Kurt and Gholami, Amir},
 journal = {arXiv preprint arXiv:2312.04511},
 title = {An {LLM} Compiler for Parallel Function Calling},
 year = {2023}
}

@article{kingmaAdamMethodStochastic2014,
 author = {Kingma, Diederik P and Ba, Jimmy},
 journal = {arXiv preprint arXiv:1412.6980},
 title = {{Adam}: A Method for Stochastic Optimization},
 year = {2014}
}

@inproceedings{larsenHowManyDegrees2022,
 author = {Larsen, Anders Boesen Lindbo and Levina, Elizaveta and Bruna, Joan and S{\o}nderby, Casper Kaae},
 booktitle = {International Conference on Machine Learning},
 title = {How Many Degrees of Freedom Do We Need to Train Deep Networks: A Loss Landscape Perspective},
 year = {2022}
}

@inproceedings{leeGradientBasedMetaLearningLearned2018,
 author = {Lee, Keuntaek and Choi, Junsoo and Shin, Jinwoo and Lee, Sung Ju Hwang},
 booktitle = {International Conference on Machine Learning},
 title = {Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace},
 year = {2018}
}

@article{lialinReLoRAHighRankTraining2023,
 author = {Lialin, Vladimir and Schatz, Arthur},
 journal = {arXiv preprint arXiv:2307.09769},
 title = {{ReLoRA}: Low-Rank Fine-Tuning Reloaded},
 year = {2023}
}

@article{liMemoryEfficientOptimizers2023,
 author = {Li, Li and Yang, Shiyu and Chen, Zhe and others},
 journal = {arXiv preprint arXiv:2302.05696},
 title = {Memory-Efficient Optimizers for Large-Scale Language Models},
 year = {2023}
}

@article{lin2022randomized,
 author = {Lin, Tianyi and Zhu, Zhihui and Mao, Yongyi},
 journal = {arXiv preprint arXiv:2209.04170},
 title = {Randomized Subspace Regularized Newton Method for Unconstrained Non-Convex Optimization},
 year = {2022}
}

@inproceedings{linDynamicMinibatchSGD2019,
 author = {Lin, Yuhong and Zhao, Chaoyue and Wu, Yongkai and Luo, Dabin and Sun, Lei and He, Bingsheng},
 booktitle = {2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS)},
 publisher = {IEEE},
 title = {Dynamic Mini-Batch {SGD} for Elastic Distributed Training},
 year = {2019}
}

@article{loshchilov2017decoupled,
 author = {Loshchilov, Ilya and Hutter, Frank},
 journal = {arXiv preprint arXiv:1711.05101},
 title = {Decoupled Weight Decay Regularization},
 year = {2017}
}

@article{loshchilovDecoupledWeightDecay2019,
 author = {Loshchilov, Ilya and Hutter, Frank},
 journal = {arXiv preprint arXiv:1711.05101},
 title = {Decoupled Weight Decay Regularization},
 year = {2017}
}

@article{lvAdaLomoLowmemoryOptimization2023,
 author = {Lv, Shuchang and Zhuang, Rui and Li, Li Erran},
 journal = {arXiv preprint arXiv:2301.12712},
 title = {{AdaLomo}: Adaptive Low-Memory Optimization for Large-Scale Deep Learning},
 year = {2023}
}

@article{lvFullParameterFinetuning2023,
 author = {Lv, Shuchang and Qin, Kai and Zhang, Wei and Li, Li Erran},
 journal = {arXiv preprint arXiv:2304.13586},
 title = {Full Parameter Fine-tuning for Large Language Models with Limited Resources},
 year = {2023}
}

@inproceedings{martens2015optimizing,
 author = {Martens, James and Grosse, Roger},
 booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML)},
 title = {Optimizing Neural Networks with Kronecker-Factored Approximate Curvature},
 year = {2015}
}

@article{martens2020new,
 author = {Martens, James},
 journal = {Journal of Machine Learning Research},
 title = {New Insights and Perspectives on the Natural Gradient Method},
 year = {2020}
}

@article{martensNewPerspectiveNatural2014,
 author = {Martens, James},
 journal = {arXiv preprint arXiv:1412.1193},
 title = {New Perspectives on the Natural Gradient Method},
 year = {2014}
}

@article{modoranuErrorFeedbackCan2023,
 author = {Modoranu, Teodor and Das, Mrinank and Huang, Po-Sen and Blundell, Charles},
 journal = {arXiv preprint arXiv:2302.04970},
 title = {Error Feedback Can Make Low-Precision Training More Robust},
 year = {2023}
}

@misc{opsahlong2024optimizinginstructionsdemonstrationsmultistage,
 archiveprefix = {arXiv},
 author = {Krista Opsahl-Ong and Michael J Ryan and Josh Purtell and David Broman and Christopher Potts and Matei Zaharia and Omar Khattab},
 eprint = {2406.11695},
 primaryclass = {cs.CL},
 title = {Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs},
 url = {https://arxiv.org/abs/2406.11695},
 year = {2024}
}

@article{raeScalingLanguageModels2021,
 author = {Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
 journal = {arXiv preprint arXiv:2112.11446},
 title = {Scaling Language Models: Methods, Analysis \& Insights from Training Gopher},
 year = {2021}
}

@article{raffelExploringLimitsTransfer2020,
 author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
 journal = {Journal of Machine Learning Research},
 title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
 year = {2020}
}

@inproceedings{rajbhandariZeROMemoryOptimizations2020,
 author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
 booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
 title = {{ZeRO}: Memory Optimizations Toward Training Trillion Parameter Models},
 year = {2020}
}

@inproceedings{rajpurkar2016squad,
 author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
 booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
 title = {{SQuAD}: 100,000+ Questions for Machine Comprehension of Text},
 year = {2016}
}

@article{renduchintalaTiedLoraEnhacingParameter2023,
 author = {Renduchintala, Adithya and Rodriguez, Pedro and Creutz, Mathias},
 journal = {arXiv preprint arXiv:2306.13420},
 title = {Tied LoRA: Enhancing Parameter-Efficient Fine-Tuning with Tied Weights},
 year = {2023}
}

@inproceedings{shazeerAdafactorAdaptiveLearning2018,
 author = {Shazeer, Noam and Stern, Mitchell},
 booktitle = {International Conference on Machine Learning},
 title = {{Adafactor}: Adaptive Learning Rates with Sublinear Memory Cost},
 year = {2018}
}

@article{shazeerGLUVariantsImprove2020,
 author = {Shazeer, Noam},
 journal = {arXiv preprint arXiv:2002.05202},
 title = {{GLU} Variants Improve Transformer},
 year = {2020}
}

@article{shengSLoRAServingThousands2023,
 author = {Sheng, Yi and Han, Xuefei and Zhu, Xuefeng and Yang, Yuanzhi and Sun, Jiani and Zhou, Guohui},
 journal = {arXiv preprint arXiv:2306.01125},
 title = {{S-LoRA}: Scalable Efficient Model Serving for Massive LoRA Models},
 year = {2023}
}

@article{shoeybiMegatronLMTuningScaling2019,
 author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Rohan and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
 journal = {arXiv preprint arXiv:1909.08053},
 title = {{Megatron-LM}: Training Multi-Billion Parameter Language Models Using Model Parallelism},
 year = {2019}
}

@article{tian2020understanding,
 author = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
 journal = {arXiv preprint arXiv:2006.08603},
 title = {Understanding Self-Supervised Learning Dynamics Without Contrastive Pairs},
 year = {2020}
}

@article{tian2023joma,
 author = {Tian, Yu and Huang, Zhen and Liu, Yifan and Ding, Minghao and Yu, Wenhu and Xie, Weidi},
 journal = {arXiv preprint arXiv:2302.12369},
 title = {{JOMA}: Joint Matrix Adaptation for Efficient Transfer Learning},
 year = {2023}
}

@article{touvronLlamaOpenFoundation2023,
 author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
 journal = {arXiv preprint arXiv:2302.13971},
 title = {{LLaMA}: Open and Efficient Foundation Language Models},
 year = {2023}
}

@article{tuckerMathematicalNotesThree1966,
 author = {Tucker, Ledyard R},
 journal = {Psychometrika},
 title = {Some Mathematical Notes on Three-Mode Factor Analysis},
 year = {1966}
}

@inproceedings{vogelsPowerGossipPracticalLowRank2020,
 author = {Vogels, Thijs and Jaggi, Martin and Patrini, Giorgio},
 booktitle = {International Conference on Machine Learning},
 title = {{PowerGossip}: Practical Low-Rank Communication for Decentralized Optimization},
 year = {2020}
}

@inproceedings{wangATOMOCommunicationefficientLearning,
 author = {Wang, Shiqiang and Joshi, Gauri and Ghosh, Sreeram K and Poor, H Vincent},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {{ATOMO}: Communication-Efficient Learning via Atomic Sparsification},
 year = {2018}
}

@article{wangCuttlefishLowrankModel2023,
 author = {Wang, Mengzhao and Liu, Zhao and Bai, Yao and Gao, Yuan},
 journal = {arXiv preprint arXiv:2305.19635},
 title = {{Cuttlefish}: Low-Rank Model Training Without Factorization},
 year = {2023}
}

@inproceedings{wangGLUEMultiTaskBenchmark2019,
 author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
 booktitle = {International Conference on Learning Representations},
 title = {{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
 year = {2019}
}

@article{wangMultiLoRADemocratizingLoRA2023,
 author = {Wang, Zihao and Bai, Zhen and Ananiadou, Sophia},
 journal = {arXiv preprint arXiv:2305.14377},
 title = {{Multi-LoRA}: Efficient Fine-Tuning for Democratic {AI}},
 year = {2023}
}

@article{warstadt2019neural,
 author = {Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},
 journal = {Transactions of the Association for Computational Linguistics},
 title = {Neural Network Acceptability Judgments},
 year = {2019}
}

@inproceedings{williams2018broad,
 author = {Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
 booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 title = {A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
 year = {2018}
}

@article{xiaChainLoRAEfficient2023,
 author = {Xia, Tianle and Gao, Xin and Yang, Jian and Wang, Xun and Wang, Liwei and Sun, Ming},
 journal = {arXiv preprint arXiv:2308.02270},
 title = {Chain-of-Thought LoRA: Efficiently Steering Large Language Models via Rank-One Model Updates},
 year = {2023}
}

@inproceedings{xiaChainLoRAEfficient2024,
 author = {Xia, Tianxiang and Peng, Hao and Chen, Zheyu and Li, Lemao and He, Zhiyuan and Yang, Zhen and Ma, Wei-Ying},
 booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 title = {Chain-of-Thought LoRA: Efficient Adaptation of Large Language Models},
 year = {2024}
}

@article{yang2023spectral,
 author = {Yang, Zhilin and Hu, Edward J and Xia, Tianle and Socher, Richard and Li, Yuanzhi},
 journal = {arXiv preprint arXiv:2305.14683},
 title = {Spectral Methods in Low-Rank Model Adaptation},
 year = {2023}
}

@misc{yang2024largelanguagemodelsoptimizers,
 archiveprefix = {arXiv},
 author = {Chengrun Yang and Xuezhi Wang and Yifeng Lu and Hanxiao Liu and Quoc V. Le and Denny Zhou and Xinyun Chen},
 eprint = {2309.03409},
 primaryclass = {cs.LG},
 title = {Large Language Models as Optimizers},
 url = {https://arxiv.org/abs/2309.03409},
 year = {2024}
}

@article{zhangLORAFAMEMORYEFFICIENTLOWRANK,
 author = {Zhang, Rui and others},
 journal = {arXiv preprint arXiv:2302.05653},
 title = {{LoRA-FA}: Memory-Efficient Low-Rank Adaptation via Feature Re-Alignment},
 year = {2023}
}

@article{zhao2024galore,
 author = {Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
 journal = {arXiv preprint arXiv:2403.03507},
 title = {{GaLore}: Memory-Efficient {LLM} Training by Gradient Low-Rank Projection},
 year = {2024}
}

@article{zhaoExtendingTorchElasticStateful2020,
 author = {Zhao, Tianshi and Sun, Zhen and Wang, Xiaodong and Zhou, Fei and Guo, Yang and Smola, Alexander J},
 journal = {arXiv preprint arXiv:2006.06873},
 title = {Extending TorchElastic for Stateful Training Jobs},
 year = {2020}
}

@article{zhaoInRankIncrementalLowRank2023,
 author = {Zhao, Yuwei and Zhang, Yifan and others},
 journal = {arXiv preprint arXiv:2303.11246},
 title = {{In-Rank}: Incremental Low-Rank Learning},
 year = {2023}
}

@article{zhaoZerOInitializationInitializing2022,
 author = {Zhao, Shangqian and Li, Shiyu and Ma, Yi},
 journal = {arXiv preprint arXiv:2207.05848},
 title = {{ZerO} Initialization: Initializing Neural Networks with Zero-Valued Parameters},
 year = {2022}
}
